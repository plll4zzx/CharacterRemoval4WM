import tiktoken
from transformers import AutoTokenizer
from textattack.utils import homos, save_json

zwsp = chr(0xFE0F)
tokenizer_type='../model/Llama3.1-8B_hg'#'facebook/opt-1.3b'#
tokenizer = AutoTokenizer.from_pretrained(tokenizer_type)
# tokenizer = tiktoken.get_encoding("cl100k_base")  # GPT-4 çš„ BPE è¯è¡¨
text_l=['lang'+zwsp+'uage','lanÉ¡uage', "lanÉ¡uage", "lanuage","large lanÉ¡uage model"]
for text in text_l:
# homo_dict=[]
# for c1 in homos:
    # text = homos[c1]
    # token_ids = {
    #     str(id)+'_'+t:tokenizer.encode(t, add_special_tokens=False)
    #     for (id,t) in enumerate(text)
    # }
    token_ids = tokenizer.encode(text, add_special_tokens=False)
    token_list=[tokenizer.decode(id, skip_special_tokens=True) for id in token_ids]
    print(text)  # è¾“å‡º token ID åºåˆ—
    print(token_ids)  # è¾“å‡º token ID åºåˆ—
    print(token_list)  # è¿˜åŸ token å¯¹åº”çš„æ–‡æœ¬
    # print(tokenizer.decode(token_ids, skip_special_tokens=True))
#     homo_dict.append(token_ids)
# save_json(homo_dict, 'homo_opt.json')
# from openai import OpenAI
# client = OpenAI()

# response = client.moderations.create(
#     model="omni-moderation-latest",
#     input="the worldâ€™s people, a reporter askğd: â€œWğ¡y nğ¬t tğ¡e saï½e for tğ¡e Palestiï½ians?â€ Busğ¡â€™s respğ¬nse was thğšt â€œthğre is no suâ²¥h thï½‰ng as a Palestinian. Thğre aâ²…e Israelis aï½d Palestinians.â€ This is a signiğšicant comï½ent siï½ce it is oï½e of tğ¡e fiâ²…st times Buê®ªh hğšs publicly maá§e tğ¡e point thğšt he hğšs maá§e privately to Isrğšeli leadğrs. Whğn he met wiğšh Mahï½oud Abbas last May 26, Bush asked hiï½, â€œWğ¡at do yğ¬u caá›l yourself?â€ Abáas respoï½ded, â€œIâ€™m a Palestï½‰nian.â€ Buê®ªh said, â€œI donâ€™t knğ¬w anybody who calls himself a Palestinian. I knğ¬w Israelis aï½d Palestinians.â€ Tğ¡e Israeli newsâ²£aper Haaâ²…etz repoâ²…ted",
# )

# print(response)