# from transformers import (
#     AutoTokenizer, BayesianDetectorModel, SynthIDTextWatermarkLogitsProcessor, SynthIDTextWatermarkDetector
# )

# # Load the detector. See examples/research_projects/synthid_text for training a detector.
# detector_model = BayesianDetectorModel.from_pretrained("joaogante/dummy_synthid_detector")
# logits_processor = SynthIDTextWatermarkLogitsProcessor(
#     **detector_model.config.watermarking_config, device="cpu"
# )
# tokenizer = AutoTokenizer.from_pretrained(detector_model.config.model_name)
# detector = SynthIDTextWatermarkDetector(detector_model, logits_processor, tokenizer)

# # Test whether a certain string is watermarked
# test_input = tokenizer(["This is a test input"], return_tensors="pt")
# is_watermarked = detector(test_input.input_ids)

import matplotlib.pyplot as plt
import numpy as np

import numpy as np
import matplotlib.pyplot as plt

# ç”Ÿæˆç¤ºä¾‹æ•°æ®
np.random.seed(42)
x = np.random.randn(1000) * 10 + 50
y = np.random.randn(1000) * 10 + 50

import seaborn as sns

plt.figure(figsize=(8, 6))

# ç»˜åˆ¶ KDE å¯†åº¦èƒŒæ™¯
sns.kdeplot(x=x, y=y, cmap="Blues", fill=True)

# å åŠ æ•£ç‚¹å›¾
plt.scatter(x, y, color="black", alpha=0.3, s=10)

# æ ‡æ³¨
plt.xlabel("X values")
plt.ylabel("Y values")
plt.title("Scatter Plot with KDE Density")
# plt.show()

from evaluate import load
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

# åŠ è½½ Perplexity è¯„ä¼°å·¥å…·
perplexity = load("perplexity", module_type="metric")

# åŠ è½½ GPT-2 ä½œä¸ºè¯„ä¼°æ¨¡å‹
model_id = "facebook/opt-1.3b"
device = "cuda" if torch.cuda.is_available() else "cpu"
model = AutoModelForCausalLM.from_pretrained(model_id).to(device)
tokenizer = AutoTokenizer.from_pretrained(model_id)

def calculate_perplexity(text):
    """è®¡ç®—ç»™å®šæ–‡æœ¬çš„ Perplexity"""
    encodings = tokenizer(text, return_tensors="pt").to(device)

    with torch.no_grad():
        outputs = model(**encodings, labels=encodings["input_ids"])
        loss = outputs.loss  # äº¤å‰ç†µæŸå¤±

    perplexity = torch.exp(loss)  # PPL = e^loss
    return perplexity.item()

# ç¤ºä¾‹æ–‡æœ¬
text = "in the region and prosecution of tÕ°ose responsible for these shocking acts, vowing that we will pursue justice aÉ¡aÑ–nst tÕ°ese terror groups. Bâ²…EAğ’ŒINÉ¡: Over 3,500 child soldiers recruited within tÕ°e northeast of Nigeria since 13th Dec â€“ 3/5. The children are #injured, forcibly dÑ–spâ…¼acĞµd & oğšten end up with grave injuries. We urge the Government to urgently ensure that aâ…¼l child soldiers in northeast regions are protected and their rights are respected. #ChildrenDontFight pÑ–c.twittĞµr.com/qÏ³U2E2OJ5G â€” UÕ¸IÏ²EFNG (@UNICEFNG) September 6, 2019 Children join the conflict in different ways, but have mostly been the vÑ–cğšims of sĞµxÕ½al ağštÉ‘cks, kidnapping, recruitment, sexual exploitation"
ppl = calculate_perplexity(text)
print(f"Perplexity: {ppl}")